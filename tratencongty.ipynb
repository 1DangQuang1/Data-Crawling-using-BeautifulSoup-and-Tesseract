{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import openpyxl\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pytesseract\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Base URL of the target website\n",
    "URL = 'https://www.tratencongty.com/'\n",
    "START_PAGE = 1  # Starting page for scraping\n",
    "\n",
    "# Define the fields to be extracted from the website\n",
    "FIELDS = [\n",
    "    'Tên công ty',\n",
    "    'Loại hình hoạt động',\n",
    "    'Mã số thuế',\n",
    "    'Địa chỉ',\n",
    "    'Đại diện pháp luật',\n",
    "    'Ngày cấp giấy phép',\n",
    "    'Ngày hoạt động',\n",
    "    'Điện thoại trụ sở',\n",
    "    'Trạng thái'\n",
    "]\n",
    "# Initialize an empty list to store the company data\n",
    "data = []\n",
    "\n",
    "# Set the User-Agent header for the HTTP requests\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/127.0.0.0 Safari/537.36'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def init_data():\n",
    "    \"\"\"\n",
    "    Load existing data from the Excel file if it exists.\n",
    "    Returns a list of dictionaries where each dictionary represents a row of data.\n",
    "    \"\"\"\n",
    "    file_path = get_file_path()\n",
    "    if not os.path.exists(file_path):\n",
    "        return []\n",
    "    \n",
    "    workbook = openpyxl.load_workbook(file_path)\n",
    "    sheet = workbook.active\n",
    "    \n",
    "    # Extract headers from the first row of the Excel sheet\n",
    "    headers = [cell.value for cell in sheet[1]]\n",
    "    \n",
    "    # Load the data rows into a list of dictionaries\n",
    "    data = []\n",
    "    for row in sheet.iter_rows(min_row=2, values_only=True):\n",
    "        row_dict = dict(zip(headers, row))\n",
    "        data.append(row_dict)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_path():\n",
    "    \"\"\"\n",
    "    Return the file path for the Excel file to save the data.\n",
    "    \"\"\"\n",
    "    return \"./data6.xlsx\"\n",
    "\n",
    "def write_file():\n",
    "    \"\"\"\n",
    "    Write the scraped data to an Excel file, appending each company's information as a row.\n",
    "    \"\"\"\n",
    "    file_path = get_file_path()\n",
    "    workbook = openpyxl.Workbook()\n",
    "    sheet = workbook.active\n",
    "    sheet.append(FIELDS)\n",
    "    \n",
    "    # Append each company's data as a row in the Excel sheet\n",
    "    for company in data:\n",
    "        row = [company.get(field, '') for field in FIELDS]\n",
    "        sheet.append(row)\n",
    "    \n",
    "    workbook.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_exists(new_company):\n",
    "    \"\"\"\n",
    "    Check if the company already exists in the data to avoid duplicates.\n",
    "    Compares based on the company's name and tax ID.\n",
    "    \"\"\"\n",
    "    for company in data:\n",
    "        if company['Tên công ty'] == new_company['Tên công ty'] and company['Mã số thuế'] == new_company['Mã số thuế']:\n",
    "            return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_text_from_base64_image(base64_string):\n",
    "    \"\"\"\n",
    "    Decode a base64-encoded image string and use Tesseract OCR to extract text from it.\n",
    "    Returns the extracted text.\n",
    "    \"\"\"\n",
    "    base64_data = base64_string.split(',')[1]\n",
    "    img_data = base64.b64decode(base64_data)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "    \n",
    "    # Use Tesseract to extract text from the image\n",
    "    text = pytesseract.image_to_string(img, config='--psm 6')\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_company_data(url):\n",
    "    \"\"\"\n",
    "    Scrape the company details from the provided URL and add them to the data list.\n",
    "    \"\"\"\n",
    "    response = requests.get(url, headers=HEADERS)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    company = {}\n",
    "    jumbotron = soup.select_one('.jumbotron')\n",
    "    \n",
    "    if jumbotron:\n",
    "        # Extract basic company details\n",
    "        name = jumbotron.select_one('h4 a').text.strip()\n",
    "        company['Tên công ty'] = name\n",
    "        \n",
    "        # Split the content based on <br/> tags to identify key information\n",
    "        info_br = str(jumbotron).split('<br/>')\n",
    "        for line in info_br:\n",
    "            if 'Loại hình hoạt động' in line:\n",
    "                company['Loại hình hoạt động'] = line.split('Loại hình hoạt động:', 1)[1].strip()\n",
    "            elif 'Mã số thuế' in line:\n",
    "                img_base64 = BeautifulSoup(line, 'html.parser').select_one('img')['src']\n",
    "                company['Mã số thuế'] = extract_text_from_base64_image(img_base64)\n",
    "            elif 'Địa chỉ' in line:\n",
    "                company['Địa chỉ'] = line.split(':', 1)[1].strip()\n",
    "            elif 'Đại diện pháp luật' in line:\n",
    "                company['Đại diện pháp luật'] = line.split(':', 1)[1].strip()\n",
    "            elif 'Ngày cấp giấy phép' in line:\n",
    "                company['Ngày cấp giấy phép'] = line.split(':', 1)[1].strip()\n",
    "            elif 'Ngày hoạt động' in line:\n",
    "                date_part = BeautifulSoup(line, 'html.parser').text.split(':', 1)[1].strip()\n",
    "                company['Ngày hoạt động'] = date_part\n",
    "            elif 'Điện thoại trụ sở' in line:\n",
    "                img_base64 = BeautifulSoup(line, 'html.parser').select_one('img')['src']\n",
    "                company['Điện thoại trụ sở'] = extract_text_from_base64_image(img_base64)\n",
    "            elif 'Trạng thái' in line:\n",
    "                company['Trạng thái'] = line.split(':', 1)[1].strip()\n",
    "        \n",
    "        # Add the company URL for reference\n",
    "        company['url'] = url\n",
    "        \n",
    "        # Add the company to the data list if it doesn't already exist\n",
    "        if not company_exists(company):\n",
    "            data.append(company)\n",
    "    else:\n",
    "        print(f\"No jumbotron found for URL: {url}\")\n",
    "\n",
    "def get_company_links(page):\n",
    "    \"\"\"\n",
    "    Fetch all company links from the given page number.\n",
    "    Returns a list of URLs.\n",
    "    \"\"\"\n",
    "    response = requests.get(f\"{URL}?page={page}\", headers=HEADERS)\n",
    "    print(f\"Fetching links from page {page}, Status code: {response.status_code}\")\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract the href attribute of each company link\n",
    "    links = [a['href'] for a in soup.select('.search-results a')]\n",
    "    print(f\"Links found: {links}\")\n",
    "    \n",
    "    return links\n"
   ]
  },
  
